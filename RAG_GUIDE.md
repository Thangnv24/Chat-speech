# ğŸ“š RAG System - Complete Guide

## Tá»•ng Quan

Há»‡ thá»‘ng RAG (Retrieval-Augmented Generation) káº¿t há»£p tÃ¬m kiáº¿m thÃ´ng tin vÃ  sinh vÄƒn báº£n Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u cá»§a báº¡n.

### Kiáº¿n TrÃºc

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG SYSTEM                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. INGESTION PHASE                                         â”‚
â”‚     Documents (PDF/TXT)                                     â”‚
â”‚            â†“                                                 â”‚
â”‚     Advanced Chunking (chunking.py)                         â”‚
â”‚       - Context-aware splitting                             â”‚
â”‚       - Math/Philosophy structure preservation              â”‚
â”‚       - Semantic clustering                                 â”‚
â”‚            â†“                                                 â”‚
â”‚     Hybrid Embeddings (ingestor.py)                         â”‚
â”‚       - Dense: sentence-transformers                        â”‚
â”‚       - Sparse: BM25 (Vietnamese tokenization)              â”‚
â”‚            â†“                                                 â”‚
â”‚     Vector Database (Qdrant)                                â”‚
â”‚       - Persistent storage                                  â”‚
â”‚       - Fast similarity search                              â”‚
â”‚                                                              â”‚
â”‚  2. RETRIEVAL PHASE                                         â”‚
â”‚     User Query                                              â”‚
â”‚            â†“                                                 â”‚
â”‚     Query Type Detection (retriever.py)                     â”‚
â”‚       - Mathematics                                         â”‚
â”‚       - Philosophy                                          â”‚
â”‚       - General                                             â”‚
â”‚            â†“                                                 â”‚
â”‚     Hybrid Search                                           â”‚
â”‚       - Dense vector search (semantic)                      â”‚
â”‚       - Sparse BM25 search (keyword)                        â”‚
â”‚       - Reciprocal Rank Fusion (RRF)                        â”‚
â”‚            â†“                                                 â”‚
â”‚     Top-K Relevant Documents                                â”‚
â”‚                                                              â”‚
â”‚  3. GENERATION PHASE                                        â”‚
â”‚     Context Preparation                                     â”‚
â”‚            â†“                                                 â”‚
â”‚     LLM Generation (Gemini/Qwen/Ollama)                     â”‚
â”‚       - Context-aware prompts                               â”‚
â”‚       - Domain-specific templates                           â”‚
â”‚            â†“                                                 â”‚
â”‚     Final Answer                                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### 1. CÃ i Äáº·t Dependencies

```bash
# Core dependencies
pip install qdrant-client langchain langchain-core
pip install sentence-transformers rank-bm25
pip install PyMuPDF scikit-learn underthesea

# Optional: LLM providers
pip install google-generativeai  # For Gemini
pip install groq                 # For Groq
```

### 2. Khá»Ÿi Äá»™ng Qdrant

```bash
# Option 1: Docker (Recommended)
docker run -p 6333:6333 qdrant/qdrant

# Option 2: Qdrant Cloud (Free tier)
# Sign up at: https://cloud.qdrant.io
```

### 3. Cáº¥u HÃ¬nh LLM

ThÃªm vÃ o `.env`:
```env
# Choose one or more
GEMINI_API_KEY=your_gemini_key
QWEN_API_KEY=your_qwen_key
GROQ_API_KEY=your_groq_key
```

### 4. Cháº¡y Demo

```bash
cd app/service/RAG
python main.py
```

---

## ğŸ“– Luá»“ng Hoáº¡t Äá»™ng Chi Tiáº¿t

### Phase 1: Document Ingestion

#### Step 1.1: Document Loading
```python
from app.service.RAG.chunking import get_chunker

chunker = get_chunker()
chunks = chunker.process_pdf("document.pdf")
```

**Xá»­ lÃ½:**
- Äá»c PDF vá»›i PyMuPDF (fitz)
- Extract text tá»« má»—i trang
- Detect document type (Math/Philosophy/General)

#### Step 1.2: Advanced Chunking

**Context-Aware Chunking:**
- **Mathematics**: Báº£o toÃ n Ä‘á»‹nh lÃ½, chá»©ng minh, cÃ´ng thá»©c
- **Philosophy**: Giá»¯ nguyÃªn luáº­n Ä‘iá»ƒm, khÃ¡i niá»‡m
- **Mixed**: Káº¿t há»£p cáº£ hai

**Techniques:**
- Regex pattern matching cho structures
- Recursive splitting vá»›i overlap
- Semantic clustering (TF-IDF + K-means)

**Output:**
```python
Document(
    page_content="Äá»‹nh lÃ½ Pythagore: aÂ² + bÂ² = cÂ²...",
    metadata={
        "chunk_id": 0,
        "document_type": "mathematics",
        "math_structures": ["theorem", "equation"],
        "contains_equations": True,
        "chunk_size": 512
    }
)
```

#### Step 1.3: Hybrid Embeddings

**Dense Embeddings:**
- Model: `sentence-transformers/all-MiniLM-L6-v2`
- Dimension: 384
- Captures semantic meaning

**Sparse Embeddings (BM25):**
- Vietnamese tokenization (underthesea)
- Term frequency analysis
- Keyword-based matching

#### Step 1.4: Vector Storage (Qdrant)

```python
from app.service.RAG.ingestor import DocumentIngestor

ingestor = DocumentIngestor(
    qdrant_url="http://localhost:6333",
    collection_name="math_philosophy"
)

vector_store = ingestor.ingest_documents([
    "data/doc_1.pdf",
    "data/doc_2.txt"
])
```

**Qdrant Features:**
- Persistent storage
- Fast HNSW indexing
- Metadata filtering
- Scalable to millions of vectors

---

### Phase 2: Retrieval

#### Step 2.1: Query Processing

```python
from app.service.RAG.retriever import create_retriever

retriever = create_retriever(vector_store, search_type="hybrid")
result = retriever.retrieve(
    query="Äá»‹nh lÃ½ Pythagore lÃ  gÃ¬?",
    k=5
)
```

**Query Type Detection:**
- Keyword matching
- Math keywords: toÃ¡n, tÃ­nh, phÆ°Æ¡ng trÃ¬nh, Ä‘á»‹nh lÃ½
- Philosophy keywords: triáº¿t, quan Ä‘iá»ƒm, há»c thuyáº¿t

#### Step 2.2: Hybrid Search

**Dense Search:**
```python
# Semantic similarity using embeddings
dense_results = vector_store._dense_search(query, k=10)
```

**Sparse Search (BM25):**
```python
# Keyword-based matching
sparse_results = vector_store._sparse_search(query, k=10)
```

**Reciprocal Rank Fusion (RRF):**
```python
# Combine results
for doc in all_docs:
    rrf_score = 1/(k + dense_rank) + 1/(k + sparse_rank)
```

**Benefits:**
- Dense: Handles synonyms, paraphrasing
- Sparse: Exact keyword matching
- RRF: Best of both worlds

#### Step 2.3: Re-ranking

- Sort by RRF score
- Filter by score threshold
- Return top-K documents

---

### Phase 3: Generation

#### Step 3.1: Context Preparation

```python
context = """
[TÃ i liá»‡u 1 - Äá»™ tin cáº­y: 0.892]
Loáº¡i: mathematics
Cáº¥u trÃºc ToÃ¡n: theorem, equation
Ná»™i dung: Äá»‹nh lÃ½ Pythagore phÃ¡t biá»ƒu ráº±ng...

[TÃ i liá»‡u 2 - Äá»™ tin cáº­y: 0.845]
...
"""
```

#### Step 3.2: Prompt Selection

**Domain-Specific Prompts:**
- Mathematics: Emphasize formulas, proofs
- Philosophy: Focus on concepts, arguments
- General: Balanced approach

```python
from app.config.prompts import PROMPT_MAP

prompt = PROMPT_MAP["mathematics"]["answer"]
formatted = prompt.format(context=context, question=query)
```

#### Step 3.3: LLM Generation

```python
from app.config.llm_config import llm_config

llm = llm_config.get_llm_client()
answer = llm(formatted_prompt)
```

**Supported LLMs:**
- **Gemini Pro**: Fast, accurate, free tier
- **Qwen**: Chinese/Vietnamese optimized
- **Ollama**: Local, private
- **Groq**: Ultra-fast inference

---

## ğŸ¯ Features

### 1. Advanced Chunking

**Context-Aware:**
- Detects document type automatically
- Preserves mathematical structures
- Maintains philosophical arguments

**Techniques:**
- Rule-based splitting (theorems, proofs)
- Semantic clustering (TF-IDF)
- Recursive splitting with overlap

### 2. Hybrid Search

**Dense + Sparse:**
- Semantic understanding (embeddings)
- Keyword matching (BM25)
- Optimal combination (RRF)

**Search Modes:**
```python
# Hybrid (recommended)
result = retriever.retrieve(query, search_mode="hybrid")

# Dense only (semantic)
result = retriever.retrieve(query, search_mode="dense")

# Sparse only (keyword)
result = retriever.retrieve(query, search_mode="sparse")
```

### 3. Multi-LLM Support

**Automatic Fallback:**
1. Try Gemini (if API key available)
2. Try Qwen (if API key available)
3. Fall back to Ollama (local)

**Configuration:**
```python
from app.config.llm_config import LLMConfig

config = LLMConfig()
config.provider  # Auto-detected: GEMINI, QWEN, or OLLAMA
```

### 4. Evaluation Metrics

**Retrieval Metrics:**
- Precision@K, Recall@K
- F1 Score
- Mean Reciprocal Rank (MRR)
- Normalized Discounted Cumulative Gain (NDCG)
- Mean Average Precision (MAP)

**Generation Metrics:**
- Answer Relevance
- Faithfulness (context adherence)
- Answer Length

**Performance Metrics:**
- Latency (mean, median, P95, P99)
- Throughput (queries/second)

```python
from app.service.RAG.rag_metrics import create_metrics

metrics = create_metrics()

# Evaluate retrieval
eval_results = metrics.evaluate_retrieval(
    retrieved_docs_list,
    relevant_docs_list,
    k_values=[1, 3, 5, 10]
)

# Evaluate generation
gen_results = metrics.evaluate_generation(
    answers, queries, contexts
)
```

### 5. Pipeline Orchestration

**Complete Workflow:**
```python
from app.service.RAG.rag_pipeline import create_pipeline

# Initialize
pipeline = create_pipeline()

# Ingest documents
stats = pipeline.ingest_documents([
    "data/doc_1.pdf",
    "data/doc_2.txt"
])

# Query
result = pipeline.query("Your question here", k=5)

# Batch processing
results = pipeline.batch_query([
    "Question 1",
    "Question 2",
    "Question 3"
])

# Health check
health = pipeline.health_check()
```

---

## ğŸ”§ Configuration

### Qdrant Settings

```python
pipeline = create_pipeline(
    qdrant_url="http://localhost:6333",  # Local
    # qdrant_url="https://xyz.cloud.qdrant.io",  # Cloud
    collection_name="my_collection"
)
```

### Chunking Parameters

```python
chunks = chunker.process_pdf(
    pdf_path="document.pdf",
    chunk_size=1000,      # Characters per chunk
    chunk_overlap=200     # Overlap between chunks
)
```

### Retrieval Parameters

```python
result = retriever.retrieve(
    query="Your question",
    k=5,                      # Number of documents
    search_mode="hybrid",     # hybrid/dense/sparse
    include_sources=True      # Include source docs
)
```

### Search Weights

```python
results = vector_store.hybrid_search(
    query="Your question",
    top_k=5,
    dense_weight=0.7,    # Weight for semantic search
    sparse_weight=0.3    # Weight for keyword search
)
```

---

## ğŸ“Š Performance

### Benchmarks (Example)

**Ingestion:**
- 100 pages PDF: ~30-60s
- Chunking: ~10-20s
- Embedding: ~20-40s
- Storage: ~5-10s

**Retrieval:**
- Query processing: ~50-200ms
- Dense search: ~20-50ms
- Sparse search: ~10-30ms
- Hybrid fusion: ~10-20ms

**Generation:**
- Gemini: ~1-3s
- Qwen: ~2-4s
- Ollama (local): ~5-15s

**Total Query Time:**
- Hybrid + Gemini: ~1.5-3.5s
- Dense + Groq: ~0.5-1.5s (fastest)

---

## ğŸ› Troubleshooting

### Qdrant Connection Failed

```bash
# Check if Qdrant is running
curl http://localhost:6333/health

# Restart Qdrant
docker restart <qdrant_container>
```

### No Documents Retrieved

**Possible causes:**
1. Collection empty â†’ Run ingestion first
2. Query too specific â†’ Try broader terms
3. Threshold too high â†’ Lower score_threshold

### LLM Not Available

**Check:**
```python
from app.config.llm_config import llm_config

print(llm_config.provider)  # Should show GEMINI/QWEN/OLLAMA
print(llm_config.config)    # Check API key
```

### Slow Performance

**Optimizations:**
1. Reduce chunk_size (faster embedding)
2. Use dense-only search (skip BM25)
3. Lower k value (fewer documents)
4. Use Groq for generation (fastest)

---

## ğŸ“š API Reference

### RAGPipeline

```python
class RAGPipeline:
    def ingest_documents(paths: List[str]) -> Dict
    def load_existing_store() -> bool
    def initialize_retriever(search_type: str) -> HybridRetriever
    def query(query: str, k: int, search_mode: str) -> Dict
    def batch_query(queries: List[str], k: int) -> List[Dict]
    def get_stats() -> Dict
    def health_check() -> Dict
```

### HybridRetriever

```python
class HybridRetriever:
    def retrieve(query: str, k: int, search_mode: str) -> Dict
    def hybrid_search(query: str, k: int) -> List[Tuple[Document, float]]
    def dense_search(query: str, k: int) -> List[Tuple[Document, float]]
    def sparse_search(query: str, k: int) -> List[Tuple[Document, float]]
```

### RAGMetrics

```python
class RAGMetrics:
    def precision_at_k(retrieved, relevant, k) -> float
    def recall_at_k(retrieved, relevant, k) -> float
    def mean_reciprocal_rank(retrieved_list, relevant_list) -> float
    def ndcg_at_k(retrieved, relevant, k) -> float
    def evaluate_retrieval(retrieved_list, relevant_list) -> Dict
    def evaluate_generation(answers, queries, contexts) -> Dict
```

---

## ğŸ“ Best Practices

### 1. Document Preparation

- **Clean PDFs**: Remove headers/footers
- **OCR if needed**: For scanned documents
- **Consistent formatting**: Better chunking

### 2. Chunking Strategy

- **Math documents**: chunk_size=800-1200
- **Philosophy**: chunk_size=1000-1500
- **General**: chunk_size=500-1000
- **Overlap**: 15-20% of chunk_size

### 3. Search Strategy

- **Precise queries**: Use dense search
- **Keyword queries**: Use sparse search
- **General queries**: Use hybrid search

### 4. LLM Selection

- **Speed priority**: Groq
- **Quality priority**: Gemini
- **Privacy priority**: Ollama (local)
- **Vietnamese**: Qwen

### 5. Evaluation

- Always evaluate on test set
- Track metrics over time
- A/B test different configurations

---

## ğŸ”® Future Enhancements

### Planned Features

1. **Multi-modal RAG**: Images, tables, charts
2. **Streaming responses**: Real-time generation
3. **Query expansion**: Automatic query refinement
4. **Re-ranking models**: Cross-encoder re-ranking
5. **Caching**: Redis cache for frequent queries
6. **Multi-language**: Better Vietnamese support
7. **Graph RAG**: Knowledge graph integration

---

## ğŸ“ Support

### Resources

- **Documentation**: This guide
- **Examples**: `app/service/RAG/main.py`
- **Logs**: `logs/app.log`, `logs/error.log`

### Common Issues

See **Troubleshooting** section above.

---

## ğŸ“„ License

This RAG system is part of the FastAPI project.

---

**Happy RAG-ing! ğŸš€**
